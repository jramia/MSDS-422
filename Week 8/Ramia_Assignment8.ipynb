{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHr_UJp8MjjT"
   },
   "source": [
    "# Ramia_Assignment8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23OWwXDXUzbc"
   },
   "source": [
    "**Introduction:** This assignment involves working with language models developed with pretrained word vectors. We use sentences (sequences of words) to train language models for predicting movie review sentiment (thumbs up versus thumbs down). We study effects of word vector size, vocabulary size, and source on classification performance. We build on resources for recurrent neural networks (RNNs) as implemented in TensorFlow. RNNs are well suited to the analysis of sequences, as needed for natural language processing (NLP).\n",
    "\n",
    "Specialized RNN models have been developed to accommodate the needs of many language processing tasks. Larger relevant vocabularies are usually associated with more accurate models, but training with larger vocabularies requires more memory and longer processing times. We can speed up the training process by using pretrained word vectors and subsets of pretrained word vectors.\n",
    "\n",
    "Technologies such as word2vec, GloVe (global vectors), and fastText provide ways of representing words as numeric vectors. These numeric vectors or neural network embeddings capture the meaning of words as well as their common usage as parts of speech. Word embeddings have extensive applications in natural language processing.\n",
    "\n",
    "Previous work involved gathering embeddings via chakin. Following methods described in https://github.com/chakki-works/chakin, the previous program, run-chakin-to-get-embeddings-v001.py downloaded pre-trained GloVe embeddings, saved them in a zip archive, and unzipped that archive to create the four word-to-embeddings text files for use in language models. \n",
    "\n",
    "In a 2x2 experimental design, we test classification performance of an RNN trained on four different GloVe embeddings. Two embeddings were sourced from Wikipedia, while the other two were sourced from Twitter. The Wikipedia embeddings have a vocabulary of 400K and dimensions of either 50 or 100. The Twitter embeddings have a vocabulary of 1.2M and dimensions of either 50 or 100. In the end we found that the model trained on Twitter embeddings with 50 dimensions performed the best on the test set, although results were a bit strange given the under-optimized nature of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIGh2Pk6MqEk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "import re  # regular expressions\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwMH2O5eoxor"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 9999\n",
    "REMOVE_STOPWORDS = False  # no stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPh4eIWeoz_j"
   },
   "outputs": [],
   "source": [
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cxESeGwjONKa",
    "outputId": "ac7455c3-053e-421c-aa12-b07337f066a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Acm0-oNVVxyZ"
   },
   "source": [
    "Define the embedding files' source directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTu42tvyguKr"
   },
   "outputs": [],
   "source": [
    "glove_6B_50d = '/content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.6B.50d.txt'\n",
    "glove_6B_100d = '/content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.6B.100d.txt'\n",
    "glove_twitter_27B_50d = '/content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.twitter.27B.50d.txt'\n",
    "glove_twitter_27B_100d = '/content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FePdz_AIe5iV"
   },
   "source": [
    "Utility function for loading embeddings follows methods described in https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer creates the Python defaultdict dictionary word_to_embedding_dict for the requested pre-trained word embeddings\n",
    "\n",
    "If with_indexes=True, we return a tuple of two dictionnaries (\"word_to_index_dict\" and \"index_to_embedding_array\"). Otherwise we return only a direct \"word_to_embedding_dict\" dictionnary mapping from a string to a numpy array.\n",
    "\n",
    "Note the use of defaultdict data structure from the Python Standard Library collections_defaultdict.py lets the caller specify a default value up front. The default value will be retuned if the key is not a known dictionary key. For word embeddings, this default value is a vector of zeros. That is, unknown words are represented by a vector of zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVTXygiLQItQ"
   },
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "w0VRhxy8f-9E",
    "outputId": "6d5c645e-7b70-42d4-ed4e-7074a5f58f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from /content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.6B.50d.txt\n",
      "Embedding loaded from disk\n",
      "Embedding is of shape: (400001, 50)\n",
      "\n",
      "Loading embeddings from /content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.6B.100d.txt\n",
      "Embedding loaded from disk\n",
      "Embedding is of shape: (400001, 100)\n",
      "\n",
      "Loading embeddings from /content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.twitter.27B.50d.txt\n",
      "Embedding loaded from disk\n",
      "Embedding is of shape: (1193515, 50)\n",
      "\n",
      "Loading embeddings from /content/gdrive/My Drive/MSDS 422/Week 8/embeddings/glove.twitter.27B.100d.txt\n",
      "Embedding loaded from disk\n",
      "Embedding is of shape: (1193515, 100)\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading embeddings from', glove_6B_50d)\n",
    "word_to_index1, index_to_embedding1 = load_embedding_from_disks(glove_6B_50d, with_indexes=True)\n",
    "print(\"Embedding loaded from disk\")\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding1.shape))\n",
    "\n",
    "print('\\nLoading embeddings from', glove_6B_100d)\n",
    "word_to_index2, index_to_embedding2 = load_embedding_from_disks(glove_6B_100d, with_indexes=True)\n",
    "print(\"Embedding loaded from disk\")\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding2.shape))\n",
    "\n",
    "print('\\nLoading embeddings from', glove_twitter_27B_50d)\n",
    "word_to_index3, index_to_embedding3 = load_embedding_from_disks(glove_twitter_27B_50d, with_indexes=True)\n",
    "print(\"Embedding loaded from disk\")\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding3.shape))\n",
    "\n",
    "print('\\nLoading embeddings from', glove_twitter_27B_100d)\n",
    "word_to_index4, index_to_embedding4 = load_embedding_from_disks(glove_twitter_27B_100d, with_indexes=True)\n",
    "print(\"Embedding loaded from disk\")\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding4.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDoRufLAtE1M"
   },
   "source": [
    "**Note:** The shape of an embedding is the number of words followed by the number of dimensions per word.\n",
    "\n",
    "The first words in an embedding are words that tend to occur more often. For unknown words, the representation is an empty vector and their index becomes the last one.\n",
    "\n",
    "To demonstrate how to use embeddings dictionaries we use the following test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1734
    },
    "colab_type": "code",
    "id": "iP1Dgimir_if",
    "outputId": "7ebd7f17-45d1-497a-b00a-f7ced474c62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the: \n",
      " index = 0 \n",
      " embedding = [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick: \n",
      " index = 2582 \n",
      " embedding = [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown: \n",
      " index = 1042 \n",
      " embedding = [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox: \n",
      " index = 2106 \n",
      " embedding = [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps: \n",
      " index = 11070 \n",
      " embedding = [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over: \n",
      " index = 74 \n",
      " embedding = [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the: \n",
      " index = 0 \n",
      " embedding = [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy: \n",
      " index = 16531 \n",
      " embedding = [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog: \n",
      " index = 2926 \n",
      " embedding = [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      len(word_to_index1), 'words:\\n')\n",
    "\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    idx = word_to_index1[word_]\n",
    "    embedding = index_to_embedding1[idx]\n",
    "    print(word_ + ':', '\\n', 'index =', idx, '\\n', 'embedding =', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE1763LdYNPt"
   },
   "source": [
    "The following code is for working with movie reviews data. \n",
    "\n",
    "Utility function to get file names within a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmSWmnDcx3DE"
   },
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P_g1RVBYuwh"
   },
   "source": [
    "Define list of codes to be dropped from the document (carriage-returns, line-feeds, tabs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tv5ngYEzrBJ"
   },
   "outputs": [],
   "source": [
    "codelist = ['\\r', '\\n', '\\t']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdb0Pdn2Y9sh"
   },
   "source": [
    "Previous analysis of a list of top terms showed a number of words, along with contractions and other word strings to drop from further analysis. Add these to the usual English stopwords to be dropped from a document collection.\n",
    "\n",
    "We will not remove stopwords in this exercise because they are important to keeping sentences intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXqvjxzizuy3"
   },
   "outputs": [],
   "source": [
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmszXdeqZjPj"
   },
   "source": [
    "Text parsing function for creating text documents.\n",
    "\n",
    "There is more we could do for data preparation (stemming, looking for contractions, possessives, etc.) but we will work with what we have in this parsing function. If we want to do stemming at a later time, we can use porter = nltk.PorterStemmer() in a construction like this:\n",
    "\n",
    "words_stemmed =  [porter.stem(word) for word in initial_words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-nSIeYnzz1S"
   },
   "outputs": [],
   "source": [
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AL_CxqsmaPJS"
   },
   "source": [
    "Gather data for 500 negative movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cLjrsIIW0gZG",
    "outputId": "9b077075-92f0-4875-cf35-86e8f14a8b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: /content/gdrive/My Drive/MSDS 422/Week 8/movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "dir_name = '/content/gdrive/My Drive/MSDS 422/Week 8/movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sa8hh2AkaTut"
   },
   "source": [
    "Read data for negative movie reviews.\n",
    "\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We then break the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "T56Bpy_b05_B",
    "outputId": "de1a82d5-28d1-46da-9062-da284066e7d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under /content/gdrive/My Drive/MSDS 422/Week 8/movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    negative_documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwREQu5naifH"
   },
   "source": [
    "Gather data for 500 positive movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "I1EhdmCp1MgL",
    "outputId": "29af4416-210a-4083-d28f-8e7e8e0daa62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: /content/gdrive/My Drive/MSDS 422/Week 8/movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "dir_name = '/content/gdrive/My Drive/MSDS 422/Week 8/movie-reviews-positive'  \n",
    "\n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_mP4geEatLJ"
   },
   "source": [
    "Read data for positive movie reviews.\n",
    "\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We then break the text into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MkovW68F2mcl",
    "outputId": "82fac486-d32d-46cf-ebd8-a11417914bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under /content/gdrive/My Drive/MSDS 422/Week 8/movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    positive_documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu9TsJVYa2uW"
   },
   "source": [
    "Convert positive/negative documents into numpy array.\n",
    "\n",
    "Note that reviews vary from 22 to 1052 words, so we use the first 20 and last 20 words of each review as our word sequences for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MWsLs8IL21jC",
    "outputId": "491d9ad0-5c9e-4f82-b061-531e1ef38f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcqaIEHobMqw"
   },
   "source": [
    "Create list of lists of lists for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2MrktSh37gT"
   },
   "outputs": [],
   "source": [
    "embeddings1 = []    \n",
    "for doc in documents:\n",
    "    embedding1 = []\n",
    "    for word in doc:\n",
    "       embedding1.append(index_to_embedding1[word_to_index1[word]]) \n",
    "    embeddings1.append(embedding1)\n",
    "    \n",
    "embeddings2 = []    \n",
    "for doc in documents:\n",
    "    embedding2 = []\n",
    "    for word in doc:\n",
    "       embedding2.append(index_to_embedding2[word_to_index2[word]]) \n",
    "    embeddings2.append(embedding2)\n",
    "    \n",
    "embeddings3 = []    \n",
    "for doc in documents:\n",
    "    embedding3 = []\n",
    "    for word in doc:\n",
    "       embedding3.append(index_to_embedding3[word_to_index3[word]]) \n",
    "    embeddings3.append(embedding3)\n",
    "    \n",
    "embeddings4 = []    \n",
    "for doc in documents:\n",
    "    embedding4 = []\n",
    "    for word in doc:\n",
    "       embedding4.append(index_to_embedding4[word_to_index4[word]]) \n",
    "    embeddings4.append(embedding4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJxTAIKJbV3O"
   },
   "source": [
    "Check on the embeddings list of list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "fzVUuBnp5Do9",
    "outputId": "114bc53e-75f6-423b-ed30-cf872e57f549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: this\n",
      "Embedding for this word:\n",
      " [ 5.3074e-01  4.0117e-01 -4.0785e-01  1.5444e-01  4.7782e-01  2.0754e-01\n",
      " -2.6951e-01 -3.4023e-01 -1.0879e-01  1.0563e-01 -1.0289e-01  1.0849e-01\n",
      " -4.9681e-01 -2.5128e-01  8.4025e-01  3.8949e-01  3.2284e-01 -2.2797e-01\n",
      " -4.4342e-01 -3.1649e-01 -1.2406e-01 -2.8170e-01  1.9467e-01  5.5513e-02\n",
      "  5.6705e-01 -1.7419e+00 -9.1145e-01  2.7036e-01  4.1927e-01  2.0279e-02\n",
      "  4.0405e+00 -2.4943e-01 -2.0416e-01 -6.2762e-01 -5.4783e-02 -2.6883e-01\n",
      "  1.8444e-01  1.8204e-01 -2.3536e-01 -1.6155e-01 -2.7655e-01  3.5506e-02\n",
      " -3.8211e-01 -7.5134e-04 -2.4822e-01  2.8164e-01  1.2819e-01  2.8762e-01\n",
      "  1.4440e-01  2.3611e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 5.3074e-01  4.0117e-01 -4.0785e-01  1.5444e-01  4.7782e-01  2.0754e-01\n",
      " -2.6951e-01 -3.4023e-01 -1.0879e-01  1.0563e-01 -1.0289e-01  1.0849e-01\n",
      " -4.9681e-01 -2.5128e-01  8.4025e-01  3.8949e-01  3.2284e-01 -2.2797e-01\n",
      " -4.4342e-01 -3.1649e-01 -1.2406e-01 -2.8170e-01  1.9467e-01  5.5513e-02\n",
      "  5.6705e-01 -1.7419e+00 -9.1145e-01  2.7036e-01  4.1927e-01  2.0279e-02\n",
      "  4.0405e+00 -2.4943e-01 -2.0416e-01 -6.2762e-01 -5.4783e-02 -2.6883e-01\n",
      "  1.8444e-01  1.8204e-01 -2.3536e-01 -1.6155e-01 -2.7655e-01  3.5506e-02\n",
      " -3.8211e-01 -7.5134e-04 -2.4822e-01  2.8164e-01  1.2819e-01  2.8762e-01\n",
      "  1.4440e-01  2.3611e-01]\n"
     ]
    }
   ],
   "source": [
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      index_to_embedding1[word_to_index1[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings1[0][0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOigOQ6wbeje"
   },
   "source": [
    "To demonstrate how the embeddings can vary by file, we do the same as above using a the second embedding file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "UYnEoERD7Edr",
    "outputId": "757dfc11-4aa0-4b72-fd52-27ddf406f122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: this\n",
      "Embedding for this word:\n",
      " [-0.57058   0.44183   0.70102  -0.41713  -0.34058   0.02339  -0.071537\n",
      "  0.48177  -0.013121  0.16834  -0.13389   0.040626  0.15827  -0.44342\n",
      " -0.019403 -0.009661 -0.046284  0.093228 -0.27331   0.2285    0.33089\n",
      " -0.36474   0.078741  0.3585    0.44757  -0.2299    0.18077  -0.6265\n",
      "  0.053852 -0.29154  -0.4256    0.62903   0.14393  -0.046004 -0.21007\n",
      "  0.48879  -0.057698  0.37431  -0.030075 -0.34494  -0.29702   0.15095\n",
      "  0.28248  -0.16578   0.076131 -0.093016  0.79365  -0.60489  -0.18874\n",
      " -1.0173    0.31962  -0.16344   0.54177   1.1725   -0.47875  -3.3842\n",
      " -0.081301 -0.3528    1.8372    0.44516  -0.52666   0.99786  -0.32178\n",
      "  0.033462  1.1783   -0.072905  0.39737   0.26166   0.33111  -0.35629\n",
      " -0.16558  -0.44382  -0.14183  -0.37976   0.28994  -0.029114 -0.35169\n",
      " -0.27694  -1.344     0.19555   0.16887   0.040237 -0.80212   0.23366\n",
      " -1.3837   -0.023132  0.085395 -0.74051  -0.073934 -0.58838  -0.085735\n",
      " -0.10525  -0.51571   0.15038  -0.16694  -0.16372  -0.22702  -0.66102\n",
      "  0.47197   0.37253 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.57058   0.44183   0.70102  -0.41713  -0.34058   0.02339  -0.071537\n",
      "  0.48177  -0.013121  0.16834  -0.13389   0.040626  0.15827  -0.44342\n",
      " -0.019403 -0.009661 -0.046284  0.093228 -0.27331   0.2285    0.33089\n",
      " -0.36474   0.078741  0.3585    0.44757  -0.2299    0.18077  -0.6265\n",
      "  0.053852 -0.29154  -0.4256    0.62903   0.14393  -0.046004 -0.21007\n",
      "  0.48879  -0.057698  0.37431  -0.030075 -0.34494  -0.29702   0.15095\n",
      "  0.28248  -0.16578   0.076131 -0.093016  0.79365  -0.60489  -0.18874\n",
      " -1.0173    0.31962  -0.16344   0.54177   1.1725   -0.47875  -3.3842\n",
      " -0.081301 -0.3528    1.8372    0.44516  -0.52666   0.99786  -0.32178\n",
      "  0.033462  1.1783   -0.072905  0.39737   0.26166   0.33111  -0.35629\n",
      " -0.16558  -0.44382  -0.14183  -0.37976   0.28994  -0.029114 -0.35169\n",
      " -0.27694  -1.344     0.19555   0.16887   0.040237 -0.80212   0.23366\n",
      " -1.3837   -0.023132  0.085395 -0.74051  -0.073934 -0.58838  -0.085735\n",
      " -0.10525  -0.51571   0.15038  -0.16694  -0.16372  -0.22702  -0.66102\n",
      "  0.47197   0.37253 ]\n"
     ]
    }
   ],
   "source": [
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      index_to_embedding2[word_to_index2[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings2[0][0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYDE8SN4b2AY"
   },
   "source": [
    "Make embeddings a numpy array for use in an RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVo9-LNd7INi"
   },
   "outputs": [],
   "source": [
    "embeddings_array1 = np.array(embeddings1)\n",
    "embeddings_array2 = np.array(embeddings2)\n",
    "embeddings_array3 = np.array(embeddings3)\n",
    "embeddings_array4 = np.array(embeddings4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FaIblNkcLrj"
   },
   "source": [
    "Define the labels to be used: 500 negative (0) and 500 positive (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EujPqkKP7c-c"
   },
   "outputs": [],
   "source": [
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYV6ZI-2cEAr"
   },
   "source": [
    "Create training and test sets with Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvFdZEvJ7iAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SNKxy4R7kaJ"
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(embeddings_array1,\n",
    "                                                        thumbs_down_up,\n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state = RANDOM_SEED)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(embeddings_array2,\n",
    "                                                        thumbs_down_up,\n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state = RANDOM_SEED)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(embeddings_array3,\n",
    "                                                        thumbs_down_up,\n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state = RANDOM_SEED)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(embeddings_array4,\n",
    "                                                        thumbs_down_up,\n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkB61-vIcX71"
   },
   "source": [
    "We use a very simple Recurrent Neural Network for this assignment. Source code available at https://github.com/ageron/handson-ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lSehqMEU8RKD",
    "outputId": "17170fa4-681c-494d-cc4f-b60537fb4e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.595\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array1.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array1.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train1.shape[0] // batch_size):          \n",
    "            X_batch = X_train1[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train1[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8m7c1rtK9Da4",
    "outputId": "b31e41e6-8825-4cd0-f4a0-12e18c6dd597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.605\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array2.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array2.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train2.shape[0] // batch_size):          \n",
    "            X_batch = X_train2[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train2[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pEF5UAAB_auK",
    "outputId": "6281ec1c-80f0-4c77-cbdf-88ad748e0527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array3.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array3.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train3.shape[0] // batch_size):          \n",
    "            X_batch = X_train3[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train3[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test3, y: y_test3})\n",
    "    print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AtdSuYEk_1Ys",
    "outputId": "1cc1cb18-0aec-4d09-a37c-1e1310eac449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.635\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array4.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array4.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train4.shape[0] // batch_size):          \n",
    "            X_batch = X_train4[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train4[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test4, y: y_test4})\n",
    "    print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkOE1VnMc3tC"
   },
   "source": [
    "**Conclusion:** As mentioned earlier, the model trained on Twitter embeddings with 50 dimensions performed the best on the test set. The other three models appear to overfit the training set and underperform on the test set. Oddly, it doesn't seem as though vocabulary size improves this, as the model using Twitter embeddings with 100 dimensions performs just as poorly as the models using Wikipedia embeddings.\n",
    "\n",
    "Dimensionality also has a bit of a confusing effect. While greater number of dimensions appears to improve performance between models using Wikipedia embeddings, the same cannot be said between models using Twitter embeddings.\n",
    "\n",
    "These confusing effects would likely be remedied by optimizing the RNN. We used a simple model here for demonstration purposes, but there are a number of way to improve the results of a neural network. These include hyperparameter tuning and regularization techniques that would help ameliorate the overfitting."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ramia_Assignment8.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
