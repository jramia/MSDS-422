{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ramia_Assignment4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I will use all explanatory variables (with the exception of neighborhood) and all 506 census tract observations from the Boston Housing Study to advise a real estate brokerage firm in its attempt to employ machine learning methods. With the median value of homes in thousands of 1970 dollars as my target variable, I will attempt to assess the market value of residential real estate. I will do so by employing three modeling methods: linear regression, ridge regression, and random forests. Evaluating these methods within a cross-validation design using root mean-squared error (RMSE) as an index of prediction error, I discovered that random forests outperformed both regression models.\n",
    "\n",
    "In an attempt to improve on the random forest model, I employed the grid search technique for optimizing hyperparameters. I also explored gradient boosting as another potential improvement on the random forest model. In the end, the gradient boosting model performed the best, being able to predict median value of homes with an accuracy of ~0.98 for the training set and ~0.93 for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Data Prep and Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed value for random number generators to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I standardize X and y variables on input, I will fit the intercept term in the models. Expect fitted values to be close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_FIT_INTERCEPT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import base packages into the namespace for this program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modeling routines from Scikit Learn packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model \n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt  # for root mean-squared error calculation\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data for the Boston Housing Study, creating data frame boston_input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_input = pd.read_csv('boston.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop neighborhood from the data being considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = boston_input.drop('neighborhood', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 Rows of Data:\n",
      "\n",
      "      crim    zn  indus  chas    nox  rooms   age     dis  rad  tax  ptratio  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
      "\n",
      "   lstat    mv  \n",
      "0   4.98  24.0  \n",
      "1   9.14  21.6  \n",
      "2   4.03  34.7  \n",
      "3   2.94  33.4  \n",
      "4   5.33  36.2  \n",
      "\n",
      "General Description:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      "crim       506 non-null float64\n",
      "zn         506 non-null float64\n",
      "indus      506 non-null float64\n",
      "chas       506 non-null int64\n",
      "nox        506 non-null float64\n",
      "rooms      506 non-null float64\n",
      "age        506 non-null float64\n",
      "dis        506 non-null float64\n",
      "rad        506 non-null int64\n",
      "tax        506 non-null int64\n",
      "ptratio    506 non-null float64\n",
      "lstat      506 non-null float64\n",
      "mv         506 non-null float64\n",
      "dtypes: float64(10), int64(3)\n",
      "memory usage: 51.5 KB\n",
      "None\n",
      "\n",
      "Descriptive Statistics:\n",
      "\n",
      "             crim          zn       indus        chas         nox       rooms  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
      "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
      "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
      "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
      "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
      "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
      "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
      "\n",
      "              age         dis         rad         tax     ptratio       lstat  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean    68.574901    3.795043    9.549407  408.237154   18.455534   12.653063   \n",
      "std     28.148861    2.105710    8.707259  168.537116    2.164946    7.141062   \n",
      "min      2.900000    1.129600    1.000000  187.000000   12.600000    1.730000   \n",
      "25%     45.025000    2.100175    4.000000  279.000000   17.400000    6.950000   \n",
      "50%     77.500000    3.207450    5.000000  330.000000   19.050000   11.360000   \n",
      "75%     94.075000    5.188425   24.000000  666.000000   20.200000   16.955000   \n",
      "max    100.000000   12.126500   24.000000  711.000000   22.000000   37.970000   \n",
      "\n",
      "               mv  \n",
      "count  506.000000  \n",
      "mean    22.528854  \n",
      "std      9.182176  \n",
      "min      5.000000  \n",
      "25%     17.025000  \n",
      "50%     21.200000  \n",
      "75%     25.000000  \n",
      "max     50.000000  \n"
     ]
    }
   ],
   "source": [
    "print('\\nFirst 5 Rows of Data:\\n')\n",
    "print(boston.head())\n",
    "\n",
    "print('\\nGeneral Description:\\n')\n",
    "print(boston.info())\n",
    "\n",
    "print('\\nDescriptive Statistics:\\n')\n",
    "print(boston.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up preliminary data for fitting the models. The last column is the median housing value response. The preceding columns are the explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = boston.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model data will be a standardized form of the preliminary model data. Standard scalers subtract the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance.\n",
    "\n",
    "It is important to transform the data so that all attributes are on a common scale (e.g. using a standard scaler) before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.\n",
    "\n",
    "As with all transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data). I use pipeline to ensure that this is done correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the set of models being evaluated. Here I will explore three models: linear regression, ridge regression, and random forests.\n",
    "\n",
    "I chose ridge regression over similar regularized regression methods such as lasso and ElasticNet. In practice, ridge regression is usually the first choice between these three models. Lasso and ElasticNet have their benefits but ridge regression is usually the widely regarded default.\n",
    "\n",
    "Further, the ridge model makes a trade-off between the simplicity of the model and its performance on the training set. How much importance the model places on simplicity versus training set performance can be specified by the user, using the alpha parameter. The default parameter alpha=1, however, the optimum setting of alpha depends on the particular dataset being used. Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization. Decreasing alpha allows the coefficients to be less restricted, and we end up with a model that resembles linear regression. However, this is at the cost of generalizability. Here I will use an alpha of 10, since that performed best in my last study.\n",
    "\n",
    "Finally, I set normalize=False because I standardize the model input data outside of the modeling method calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Linear_Regression', 'Ridge_Regression', 'Random_Forest']\n",
    "\n",
    "regressors = [LinearRegression(fit_intercept = SET_FIT_INTERCEPT,\n",
    "                               normalize = False), \n",
    "              Ridge(alpha = 10, solver = 'cholesky',\n",
    "                    fit_intercept = SET_FIT_INTERCEPT,\n",
    "                    normalize = False,\n",
    "                    random_state = RANDOM_SEED),\n",
    "              RandomForestRegressor(n_estimators = 100,\n",
    "                                    random_state = RANDOM_SEED)\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the k-fold cross-validation design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten-fold cross-validation is employed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up numpy array for storing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = np.zeros((N_FOLDS, len(names)))\n",
    "\n",
    "kf = KFold(n_splits = N_FOLDS, shuffle=False, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the splitting process by looking at fold observation counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_for_fold = 0  # fold count initialized \n",
    "for train_index, test_index in kf.split(model_data):\n",
    "    X_train = model_data[train_index, 0:-1]\n",
    "    X_test = model_data[test_index, 0:-1]\n",
    "    y_train = model_data[train_index, -1]\n",
    "    y_test = model_data[test_index, -1]   \n",
    "    \n",
    "    index_for_method = 0  # initialize\n",
    "    for name, reg_model in zip(names, regressors):\n",
    "        pipeline = make_pipeline(StandardScaler(),\n",
    "                                 reg_model)\n",
    "        pipeline.fit(X_train, y_train)  # fit on the train set for this fold\n",
    "        y_test_predict = pipeline.predict(X_test) # evaluate on the test set for this fold\n",
    "        fold_method_result = sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "        cv_results[index_for_fold, index_for_method] = fold_method_result\n",
    "        index_for_method += 1\n",
    "  \n",
    "    index_for_fold += 1\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "cv_results_df.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------\n",
      "Average results from 10-fold cross-validation\n",
      "in standardized units (mean 0, standard deviation 1)\n",
      "\n",
      "Method               Root mean-squared error\n",
      "Linear_Regression    5.154728\n",
      "Ridge_Regression     5.060819\n",
      "Random_Forest        4.179909\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('\\n----------------------------------------------')\n",
    "print('Average results from ', N_FOLDS, '-fold cross-validation\\n',\n",
    "      'in standardized units (mean 0, standard deviation 1)\\n',\n",
    "      '\\nMethod               Root mean-squared error', sep = '')     \n",
    "print(cv_results_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the 10-fold cross-validation, it appears as though the random forest model provides the lowest root mean squared error (RMSE) among all of the models tested. This model is able to predict the target variable (mv) with a 4.18 RMSE, on average.\n",
    "\n",
    "Although the random forest was superior, I believe there is potential to further improve it's accuracy by optimizing hyperparameters. I will explore this below using random grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random grid by defining the options for:\n",
    "    1) Number of trees in random forest\n",
    "    2) Number of features to consider at every split\n",
    "    3) Maximum number of levels in tree\n",
    "    4) Minimum number of samples required to split a node\n",
    "    5) Minimum number of samples required at each leaf node\n",
    "    6) Method of selecting samples for training each tree (This will always be True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
      " 'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500]}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 9)]\n",
    "max_features = [int(x) for x in np.linspace(start = 1, stop = 12, num = 12)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the random grid to search for best hyperparameters using 10 fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   49.4s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500], 'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True]},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = model_data[:,0:-1]\n",
    "y = model_data[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_SEED)\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Next search across 100 different combinations and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100,\n",
    "                               cv = 10,\n",
    "                               verbose = 2,\n",
    "                               random_state = RANDOM_SEED,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the best parameters from fitting the random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 5,\n",
       " 'max_depth': 60,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if random search yielded a better model, I compare the base model with the best random search model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy\n",
      "Training: 0.980\n",
      "Test: 0.904 \n",
      "\n",
      "Best Random Accuracy\n",
      "Training: 0.984\n",
      "Test: 0.903\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestRegressor(n_estimators = 100, random_state = RANDOM_SEED)\n",
    "base_model.fit(X_train, y_train)\n",
    "print(\"Base Model Accuracy\")\n",
    "print(\"Training: {:.3f}\".format(base_model.score(X_train, y_train)))\n",
    "print(\"Test: {:.3f}\".format(base_model.score(X_test, y_test)),\"\\n\")\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "print(\"Best Random Accuracy\")\n",
    "print(\"Training: {:.3f}\".format(best_random.score(X_train, y_train)))\n",
    "print(\"Test: {:.3f}\".format(best_random.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I achieved an unspectacular improvement in accuracy on the training data of 0.004 and actually decreased in accuracy on the test data! Further, it appears as though both of these models are overfitting the training data. Hopefully gradient boosting can improve on these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy\n",
      "Training: 0.980\n",
      "Test: 0.926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Accuracy\")\n",
    "print(\"Training: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Test: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it appears as though overfitting may still be a slight issue, I think it's safe to conclude that gradient boosting is the model that should be recommended to the real estate agency.\n",
    "\n",
    "Let's take a look at the variables that the gradient boosting model determined to be the most important in predicting median value of homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         importance\n",
      "lstat      0.480762\n",
      "rooms      0.286707\n",
      "dis        0.097327\n",
      "ptratio    0.045325\n",
      "nox        0.041528\n",
      "crim       0.018002\n",
      "tax        0.012792\n",
      "age        0.012352\n",
      "indus      0.003599\n",
      "rad        0.000981\n",
      "zn         0.000425\n",
      "chas       0.000199\n"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(gbrt.feature_importances_,\n",
    "                                   index = boston.columns[0:-1],\n",
    "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these variables and the models demonstrated here, the real estate agency will be able to make smarter investments in the Boston housing market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
